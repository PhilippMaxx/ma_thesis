__call__: theory2practice.experiments.Experiment
checkpoint_freq: 1
config:
    __spec__:
        __call__: theory2practice.experiments.TrainableConfig
        algorithm:
            __call__: theory2practice.algorithms.GdAlgorithm
            data_loader_kwargs:
                batch_size:
                    eval: int(np.ceil(spec.config.n_samples * 0.2))
                shuffle: true
            distribution_wrapper: &id001
                __call__: theory2practice.utils.DistributionWrapper
                distribution_factory:
                    __import__: torch.distributions.uniform.Uniform
                tensor_kwargs:
                    high: 0.5
                    low:
                        __call__: theory2practice.utils.create_tensor
                        data:
                        - -0.5
            epochs_per_iteration: 10
            loss:
                __call__: theory2practice.utils.LpLoss
                p: 2
            model:
                __call__: theory2practice.algorithms.FeedForward
                activation:
                    __call__: torch.nn.ReLU
                depth: 3
                input_dim: 1
                width: 50
            optimizer_factory:
                __import__: torch.optim.Adam
            optimizer_kwargs:
                lr: 0.01
            scheduler_factory:
                __import__: torch.optim.lr_scheduler.ExponentialLR
            scheduler_kwargs:
                gamma:
                    eval: (1e-06 / spec.config.algorithm.optimizer_kwargs.lr) ** (1
                        / 50)
            scheduler_step_unit: epoch
            standardize: true
        dim: 1
        dtype:
            __import__: torch.double
        log_path: results/test
        n_samples: 100
        plot_save_dir: plots
        plot_x:
            __call__: theory2practice.utils.create_tensor
            creation_op:
                __import__: torch.linspace
            end: 0.5
            methods:
            -   args:
                - -1
                name: unsqueeze
            start: -0.5
            steps: 1000
        plot_x_axis:
            __call__: theory2practice.utils.create_tensor
            creation_op:
                __import__: torch.linspace
            end: 0.5
            start: -0.5
            steps: 1000
        seed: 10000
        target_fn:
            __call__: theory2practice.utils.sinusoidal_target
            factor: 1
        test_batch_size: 1024
        test_batches: 2
        test_distribution_wrapper: *id001
        test_losses:
        -   __call__: theory2practice.utils.LpLoss
            p: 1
        -   __call__: theory2practice.utils.LpLoss
            p: 2
        -   __call__: theory2practice.utils.LpLoss
            p: infinity
        wandb:
            group: test
            project: t2p_test
local_dir: results/test
max_failures: 1
name: exp
preprocessors:
-   __call__: theory2practice.experiments.SetupEnviron
    environ_update:
        TUNE_GLOBAL_CHECKPOINT_S: '600'
        TUNE_MAX_PENDING_TRIALS_PG: '1'
        TUNE_RESULT_BUFFER_LENGTH: '0'
        TUNE_WARN_THRESHOLD_S: '2'
-   __call__: theory2practice.experiments.LogSpecs
    log_path: results/test
    spec_dir: specs/test
resources_per_trial:
    cpu: 1
    gpu: 0
run:
    __import__: theory2practice.experiments.Trainable1
stop:
    training_iteration: 5
